### github
HazyResearch/flash-attention: Fast and memory-efficient exact attention
https://github.com/HazyResearch/flash-attention/tree/main

triton/python/tutorials at main · openai/triton · GitHub
https://github.com/openai/triton/tree/main/python/tutorials

triton/python/triton/ops at main · openai/triton · GitHub
https://github.com/openai/triton/tree/main/python/triton/ops

GPTQ-triton/src/gptq_triton/fused_attention.py at main · fpgaminer/GPTQ-triton · GitHub
https://github.com/fpgaminer/GPTQ-triton/blob/main/src/gptq_triton/fused_attention.py

qwopqwop200/GPTQ-for-LLaMa: 4 bits quantization of LLaMA using GPTQ
https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/triton

Optimizing Compute Shaders for L2 Locality using Thread-Group ID Swizzling | NVIDIA Technical Blog
https://developer.nvidia.com/blog/optimizing-compute-shaders-for-l2-locality-using-thread-group-id-swizzling/

kernl/src/kernl/implementations at main · ELS-RD/kernl · GitHub
https://github.com/ELS-RD/kernl/tree/main/src/kernl/implementations

### 文档
谈谈对OpenAI Triton的一些理解 - 知乎
https://zhuanlan.zhihu.com/p/613244988

OpenAI Triton：25行代码实现cuBLAS GEMM 95%以上的性能 - 知乎
https://zhuanlan.zhihu.com/p/527937835

Introducing Triton: Open-source GPU programming for neural networks
https://openai.com/research/triton

### Issues
Fix AutoTuner error when OutOfResources by aranku · Pull Request #1208 · openai/triton
https://github.com/openai/triton/pull/1208

questions about matmul implementation using triton · Issue #1869 · openai/triton
https://github.com/openai/triton/issues/1869

Performance degradation occurs when using the latest version of triton (2023.6.16) · Issue #1794 · openai/triton
https://github.com/openai/triton/issues/1794

triton.language.where return float32 always even if input is float16 · Issue #1891 · openai/triton
https://github.com/openai/triton/issues/1891

Higher register spilling when using block pointers · Issue #1830 · openai/triton
https://github.com/openai/triton/issues/1830
