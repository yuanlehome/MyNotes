{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一、jieba简介\n",
    "\n",
    "jieba库是一款优秀的 Python 第三方中文分词库，jieba 支持三种分词模式：精确模式、全模式和搜索引擎模式，下面是三种模式的特点。\n",
    "\n",
    "精确模式：\n",
    "试图将语句最精确的切分，不存在冗余数据，适合做文本分析。\n",
    "jieba.lcut（text, cut_all=False）\n",
    "\n",
    "全模式：\n",
    "将语句中所有可能是词的词语都切分出来，速度很快，但是存在冗余数据。\n",
    "jieba.lcut(text, cut_all=True)\n",
    "\n",
    "搜索引擎模式：\n",
    "在精确模式的基础上，对长词再次进行切分，提高召回率，适合用于搜索引擎分词。jieba.lcut_for_search(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.638 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['南京', '东南大学']\n",
      "['南京', '京东', '东南', '东南大学', '南大', '大学']\n",
      "['南京', '东南', '南大', '大学', '东南大学']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n全模式和搜索引擎模式的区别， 全模式是按照逐字遍历作为词语第一个字的方式； 搜索引擎模式只会对精确模式结果中长的词，再按照全模式切分一遍。\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1、切分方法 就是上述三种\n",
    "\n",
    "import jieba\n",
    "tokenizer = jieba.Tokenizer()\n",
    "text = '南京东南大学'\n",
    "print(tokenizer.lcut(text))  # 默认是精确模式，把句子拆开，返回一个list\n",
    "\n",
    "print(tokenizer.lcut(text, cut_all=True))  # 全模式，把所有可能的切分都列出来\n",
    "\n",
    "\n",
    "print(tokenizer.lcut_for_search(text))  # 搜索引擎模式\n",
    "'''\n",
    "全模式和搜索引擎模式的区别， 全模式是按照逐字遍历作为词语第一个字的方式； 搜索引擎模式只会对精确模式结果中长的词，再按照全模式切分一遍。\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2、向切分依据的字典中添加、删除词语\n",
    "'''\n",
    "jieba.add_word(word), 添加新的词语，当我们需要自定义某些名称时需要用到，如下。\n",
    "\n",
    "jieba.del_word(word), 删除字典中已有的词语\n",
    "'''\n",
    "text = '欢迎报考南京影之诗大学'\n",
    "print(jieba.lcut(text, cut_all=True))  # 可以发现没有影之诗，和影之诗大学\n",
    "# output ['欢迎', '报考', '南京', '影', '之', '诗', '大学']\n",
    "\n",
    "jieba.add_word('影之诗')\n",
    "jieba.add_word('影之诗大学')\n",
    "print(jieba.lcut(text, cut_all=True))  # 可以发现切分更准确了\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '来自', '奥力安费', '的', '哈比虾', '大学']\n"
     ]
    }
   ],
   "source": [
    "# 3、添加用户自定义词典\n",
    "'''\n",
    "jieba.load_userdict()\n",
    "\n",
    "这个作用和上面单个添加的作用一样，不过正这个是大批量添加，而且这个还可以增加描述性的赐予的词性。\n",
    "'''\n",
    "jieba.load_userdict(\"extra_word.txt\")   # 加载词典\n",
    "print(jieba.lcut(\"我来自奥力安费的哈比虾大学\",cut_all=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['废话', '你好']\n"
     ]
    }
   ],
   "source": [
    "# 4、使用停用词\n",
    "\n",
    "'''\n",
    "停用词就是那些语气词，口头禅之类的，对于研究并无实际贡献，需要删除。 \n",
    "作用原理就是在分词之后，手动遍历分词结果，看他是不是在停用词列表中，如果在，就把他删除。 可以选择手动删除，也可以使用jieba.analyse里的函数。\n",
    "'''\n",
    "from jieba import analyse\n",
    "text = '你好啊，我不讲废话啊'\n",
    "analyse.set_stop_words('stop_sord.txt')\n",
    "processed_word = analyse.extract_tags(text)\n",
    "print(processed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['二次元', '人', '就', '该', '有', '二次元', '人该', '有', '的', '样子']\n",
      "{'二次元': 2, '人': 1, '就': 1, '该': 1, '有': 2, '人该': 1, '的': 1, '样子': 1}\n"
     ]
    }
   ],
   "source": [
    "# 5、统计切分结果中的词频\n",
    "\n",
    "# 这个方法不是jieba中的功能，而是在collections包中的Counter方法， 作用是统计每个词的频数。\n",
    "\n",
    "from collections import Counter\n",
    "text = '二次元人就该有二次元人该有的样子'\n",
    "words = jieba.lcut(text, cut_all=False)\n",
    "\n",
    "\n",
    "print(words)\n",
    "print(dict(Counter(words)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
